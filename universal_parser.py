#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ—Å—Ç –≤ —Å—Ç–∞—Ç—å—è—Ö
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—Ç
"""

import json
import re
from typing import Dict, List, Optional

import requests
from bs4 import BeautifulSoup


class UniversalPlaceParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def parse_article(self, url: str, limit: Optional[int] = None) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å—Ç–∞—Ç—å—é –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–µ—Å—Ç–∞
        """
        try:
            print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏: {url}")
            
            # –ü–æ–ª—É—á–∞–µ–º HTML
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
            places = []
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º (div —Å –∫–ª–∞—Å—Å–æ–º card)
            places.extend(self._parse_card_based(soup, limit))
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º (h2, h3)
            if not places:
                places.extend(self._parse_heading_based(soup, limit))
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ø–æ —Å–ø–∏—Å–∫–∞–º (ul, ol)
            if not places:
                places.extend(self._parse_list_based(soup, limit))
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
            if not places:
                places.extend(self._parse_paragraph_based(soup, limit))
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ü–æ–∏—Å–∫ –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ –º–µ—Å—Ç–∞
            if not places:
                places.extend(self._parse_link_based(soup, limit))
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if limit and len(places) > limit:
                places = places[:limit]
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–µ—Å—Ç: {len(places)}")
            return places
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
            return []
    
    def _parse_card_based(self, soup: BeautifulSoup, limit: Optional[int] = None) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º (div —Å –∫–ª–∞—Å—Å–æ–º card)"""
        places = []
        
        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –ø–æ –∫–ª–∞—Å—Å—É
        cards = soup.find_all('div', class_='card')
        if not cards:
            # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–∞—Ä—Ç–æ—á–µ–∫
            cards = soup.find_all('div', class_=re.compile(r'card|item|place|venue|restaurant|cafe|bar'))
        
        for card in cards:
            place = self._extract_place_from_card(card)
            if place:
                places.append(place)
                if limit and len(places) >= limit:
                    break
        
        return places
    
    def _parse_heading_based(self, soup: BeautifulSoup, limit: Optional[int] = None) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º"""
        places = []
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ h2, h3, h4
        headings = soup.find_all(['h2', 'h3', 'h4'])
        
        for heading in headings:
            text = heading.get_text(strip=True)
            if self._is_place_heading(text):
                place = self._extract_place_from_heading(heading)
                if place:
                    places.append(place)
                    if limit and len(places) >= limit:
                        break
        
        return places
    
    def _parse_list_based(self, soup: BeautifulSoup, limit: Optional[int] = None) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ —Å–ø–∏—Å–∫–∞–º"""
        places = []
        
        # –ò—â–µ–º —Å–ø–∏—Å–∫–∏
        lists = soup.find_all(['ul', 'ol'])
        
        for list_elem in lists:
            items = list_elem.find_all('li')
            for item in items:
                place = self._extract_place_from_list_item(item)
                if place:
                    places.append(place)
                    if limit and len(places) >= limit:
                        break
            if limit and len(places) >= limit:
                break
        
        return places
    
    def _parse_paragraph_based(self, soup: BeautifulSoup, limit: Optional[int] = None) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –º–µ—Å—Ç"""
        places = []
        
        # –ò—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = soup.find_all('p')
        
        for p in paragraphs:
            text = p.get_text(strip=True)
            if self._contains_place_name(text):
                place = self._extract_place_from_paragraph(p)
                if place:
                    places.append(place)
                    if limit and len(places) >= limit:
                        break
        
        return places
    
    def _parse_link_based(self, soup: BeautifulSoup, limit: Optional[int] = None) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ –º–µ—Å—Ç–∞"""
        places = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏
        links = soup.find_all('a', href=True)
        
        for link in links:
            text = link.get_text(strip=True)
            href = link.get('href', '')
            
            if self._is_place_link(text, href):
                place = self._extract_place_from_link(link)
                if place:
                    places.append(place)
                    if limit and len(places) >= limit:
                        break
        
        return places
    
    def _extract_place_from_card(self, card) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Å—Ç–µ –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏"""
        try:
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = card.find(['h2', 'h3', 'h4', 'h5'])
            title = title_elem.get_text(strip=True) if title_elem else ""
            
            # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            desc_elem = card.find('p')
            description = desc_elem.get_text(strip=True) if desc_elem else ""
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img_elem = card.find('img')
            image_url = img_elem.get('src', '') if img_elem else ""
            
            # –ò—â–µ–º –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            rating = self._extract_rating(card)
            price = self._extract_price(card)
            category = self._extract_category(card)
            
            if title:
                return {
                    'title': title,
                    'description': description,
                    'image_url': image_url,
                    'rating': rating,
                    'price': price,
                    'category': category,
                    'source': 'card'
                }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
        
        return None
    
    def _extract_place_from_heading(self, heading) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Å—Ç–µ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        try:
            title = heading.get_text(strip=True)
            
            # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
            next_elem = heading.find_next_sibling()
            description = ""
            if next_elem and next_elem.name == 'p':
                description = next_elem.get_text(strip=True)
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä—è–¥–æ–º
            img_elem = heading.find_next('img')
            image_url = img_elem.get('src', '') if img_elem else ""
            
            if title:
                return {
                    'title': title,
                    'description': description,
                    'image_url': image_url,
                    'rating': None,
                    'price': None,
                    'category': None,
                    'source': 'heading'
                }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
        
        return None
    
    def _extract_place_from_list_item(self, item) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Å—Ç–µ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å–ø–∏—Å–∫–∞"""
        try:
            text = item.get_text(strip=True)
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É
            link = item.find('a')
            if link:
                title = link.get_text(strip=True)
            else:
                title = text
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img_elem = item.find('img')
            image_url = img_elem.get('src', '') if img_elem else ""
            
            if title:
                return {
                    'title': title,
                    'description': text,
                    'image_url': image_url,
                    'rating': None,
                    'price': None,
                    'category': None,
                    'source': 'list_item'
                }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑ —Å–ø–∏—Å–∫–∞: {e}")
        
        return None
    
    def _extract_place_from_paragraph(self, p) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Å—Ç–µ –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞"""
        try:
            text = p.get_text(strip=True)
            
            # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Å—Ç–∞ (–æ–±—ã—á–Ω–æ –≤ –Ω–∞—á–∞–ª–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞)
            lines = text.split('.')
            if lines:
                title = lines[0].strip()
                description = '. '.join(lines[1:]).strip() if len(lines) > 1 else ""
                
                if title:
                    return {
                        'title': title,
                        'description': description,
                        'image_url': "",
                        'rating': None,
                        'price': None,
                        'category': None,
                        'source': 'paragraph'
                    }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞: {e}")
        
        return None
    
    def _extract_place_from_link(self, link) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Å—Ç–µ –∏–∑ —Å—Å—ã–ª–∫–∏"""
        try:
            title = link.get_text(strip=True)
            href = link.get('href', '')
            
            if title and self._is_place_name(title):
                return {
                    'title': title,
                    'description': f"–°—Å—ã–ª–∫–∞: {href}",
                    'image_url': "",
                    'rating': None,
                    'price': None,
                    'category': None,
                    'source': 'link'
                }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑ —Å—Å—ã–ª–∫–∏: {e}")
        
        return None
    
    def _is_place_heading(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –º–µ—Å—Ç–∞"""
        place_indicators = [
            'restaurant', 'cafe', 'bar', 'pub', 'club', 'spa', 'hotel', 'museum',
            'gallery', 'park', 'market', 'shop', 'store', 'mall', 'cinema', 'theater',
            'restaurant', 'caf√©', 'bistro', 'diner', 'eatery', 'grill', 'steakhouse'
        ]
        
        text_lower = text.lower()
        return any(indicator in text_lower for indicator in place_indicators)
    
    def _contains_place_name(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π
        return len(text) > 10 and any(c.isupper() for c in text)
    
    def _is_place_link(self, text: str, href: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ –º–µ—Å—Ç–æ"""
        if not text or len(text) < 3:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø–æ—Ö–æ–∂ –Ω–∞ —Å–ª—É–∂–µ–±–Ω—É—é —Å—Å—ã–ª–∫—É
        service_words = ['home', 'about', 'contact', 'privacy', 'terms', 'login', 'signup']
        if text.lower() in service_words:
            return False
        
        return True
    
    def _is_place_name(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ–º –º–µ—Å—Ç–∞"""
        if not text or len(text) < 3:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
        service_words = ['home', 'about', 'contact', 'privacy', 'terms', 'login', 'signup', 'menu']
        if text.lower() in service_words:
            return False
        
        return True
    
    def _extract_rating(self, element) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò—â–µ–º –ø–æ –∫–ª–∞—Å—Å—É rating
            rating_elem = element.find(class_=re.compile(r'rating|score|stars'))
            if rating_elem:
                text = rating_elem.get_text(strip=True)
                # –ò—â–µ–º —á–∏—Å–ª–æ
                match = re.search(r'(\d+\.?\d*)', text)
                if match:
                    return float(match.group(1))
            
            # –ò—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
            text = element.get_text()
            match = re.search(r'(\d+\.?\d*)\s*stars?', text, re.IGNORECASE)
            if match:
                return float(match.group(1))
                
        except Exception:
            pass
        
        return None
    
    def _extract_price(self, element) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò—â–µ–º –ø–æ –∫–ª–∞—Å—Å—É price
            price_elem = element.find(class_=re.compile(r'price|cost'))
            if price_elem:
                return price_elem.get_text(strip=True)
            
            # –ò—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
            text = element.get_text()
            match = re.search(r'(\$+|\‚Ç¨+|\¬£+)', text)
            if match:
                return match.group(1)
                
        except Exception:
            pass
        
        return None
    
    def _extract_category(self, element) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò—â–µ–º –ø–æ –∫–ª–∞—Å—Å—É category
            cat_elem = element.find(class_=re.compile(r'category|type|genre'))
            if cat_elem:
                return cat_elem.get_text(strip=True)
            
            # –ò—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
            text = element.get_text()
            categories = ['restaurant', 'cafe', 'bar', 'spa', 'hotel', 'museum', 'gallery']
            for cat in categories:
                if cat in text.lower():
                    return cat.title()
                
        except Exception:
            pass
        
        return None
    
    def save_to_json(self, places: List[Dict], filename: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Å—Ç–∞ –≤ JSON —Ñ–∞–π–ª"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(places, f, ensure_ascii=False, indent=2)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {filename}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")

def main():
    parser = UniversalPlaceParser()
    
    # URL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_url = "https://www.timeout.com/bangkok/restaurants/bangkoks-best-new-cafes-of-2025"
    
    print("üöÄ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –º–µ—Å—Ç")
    print("=" * 50)
    
    # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å—é
    places = parser.parse_article(test_url, limit=10)
    
    if places:
        print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ –º–µ—Å—Ç: {len(places)}")
        print("-" * 50)
        
        for i, place in enumerate(places, 1):
            print(f"{i}. {place['title']}")
            if place['description']:
                print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {place['description'][:100]}...")
            if place['image_url']:
                print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {place['image_url']}")
            if place['rating']:
                print(f"   –†–µ–π—Ç–∏–Ω–≥: {place['rating']}")
            if place['price']:
                print(f"   –¶–µ–Ω–∞: {place['price']}")
            if place['category']:
                print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {place['category']}")
            print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {place['source']}")
            print()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        parser.save_to_json(places, 'parsed_places.json')
        
    else:
        print("\n‚ùå –ú–µ—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

if __name__ == "__main__":
    main()
